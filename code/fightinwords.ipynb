{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhD3FsN2U1KZG5YQUS+/dG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install unidecode"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VfcOGX4ZhFfh","executionInfo":{"status":"ok","timestamp":1662416893415,"user_tz":240,"elapsed":4526,"user":{"displayName":"Federica Bologna","userId":"16530074403271623482"}},"outputId":"725d5cae-f52c-4e51-941a-1371d5705e64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting unidecode\n","  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 6.6 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.4\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","import glob\n","import re\n","from unidecode import unidecode\n","import nltk\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Patch\n","from matplotlib.lines import Line2D\n","import matplotlib as mpl\n","from sklearn.feature_extraction.text import CountVectorizer as CV\n","\n","# Mount the Google drive for access to files\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"zJxggz4I5mi2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["incerto_dir = '/content/drive/MyDrive/incerto-autore/'\n","poems_dir = os.path.join(incerto_dir, 'poems')\n","figures_dir = os.path.join(incerto_dir, 'figures')\n","\n","poems_files = glob.glob(os.path.join(poems_dir, \"*.txt\"))"],"metadata":{"id":"PQLjyeSGx5B2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["poems_d = {'poem_label':[], 'author': [], 'poem':[]}\n","\n","for f in poems_files:\n","  author_name = f.split('/')[-1].split('_')[0]\n","  poems_d['author'].append(author_name)\n","\n","  poem_l = []\n","  with open(f, 'r', encoding='utf-8-sig') as f:\n","      for line in f:\n","        line = line.strip().lower()\n","        line = re.sub(r'[^\\w\\s]', ' ', line)\n","        line = unidecode(line, 'utf-8')\n","        poem_l.append(line)\n","\n","  poem = ' '.join(poem_l)\n","  poems_d['poem'].append(poem)\n","\n","  poem_label = poem[:5]\n","  poems_d['poem_label'].append(poem_label)"],"metadata":{"id":"cOQA1TQGx47p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["poems_df = pd.DataFrame(poems_d)\n","poems_df[-10:]"],"metadata":{"id":"rlYCcYX4ek5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def cleaning_docs(df, df_ids, df_docs):\n","#     for index, row in df.iterrows():\n","#         doc_id = row[df_ids]\n","#         doc = row[df_docs]\n","#         clean_doc = []\n","#         lowercase = doc.lower()\n","#         tokenized = nltk.word_tokenize(lowercase)  # list of tokens\n","#         for token in tokenized:\n","#             token = re.sub(r'[^\\w\\s\\d]', '', token)  # remove punctuation from token\n","#             #if token and token not in stopwords:  # if token is not empty and is not in stopwords\n","#             token = lemmatizer.lemmatize(token)  # lemmatize token\n","#             clean_doc.append(token)\n","#             vocab.add(token)\n","#         n_tokxdoc.append(len(clean_doc))\n","#         clean_doc = ' '.join(clean_doc)\n","#         if clean_doc:\n","#             doc_ids.append(doc_id)\n","#             clean_docs.append(clean_doc)\n","\n","#     print(\"Number of Documents: {}\".format(len(clean_docs)))\n","#     print(\"Mean Number of Words per Document: {}\".format(np.mean(n_tokxdoc)))\n","#     print(\"Vocabulary Size: {}\".format(len(list(vocab))))\n","\n","#     return doc_ids, clean_docs"],"metadata":{"id":"vu4N1yyReHhv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"weM3JdBCmtI9"},"outputs":[],"source":["# Monroe's fightin' words calculation\n","# Based on Jack Hessel's and Xanda Schofield's fightin words implementations\n","\n","def basic_sanitize(in_string):\n","    '''Returns a very roughly sanitized version of the input string.'''\n","    return_string = ''.join([ch for ch in in_string if ord(ch) < 128]).lower()\n","    return_string = ' '.join(return_string.split())\n","    return return_string"]},{"cell_type":"code","source":["def bayes_compare_language(lang1, lang2, output_path, ngram=1, prior=.01, cv=None, sig_val=2.573):\n","    '''\n","    Arguments:\n","    - l1, l2; a list of strings from each language sample\n","    - ngram; an int describing up to what n gram you want to consider (1 is unigrams,\n","    2 is bigrams + unigrams, etc). Ignored if a custom CountVectorizer is passed.\n","    - prior; either a float describing a uniform prior, or a vector describing a prior\n","    over vocabulary items. If you're using a predefined vocabulary, make sure to specify that\n","    when you make your CountVectorizer object.\n","    - cv; a sklearn.feature_extraction.text.CountVectorizer object, if desired.\n","    Returns:\n","    - A list of length |Vocab| where each entry is a (n-gram, zscore) tuple.'''\n","\n","    l1 = lang_d[lang1]\n","    l2 = lang_d[lang2]\n","\n","    if cv is None and type(prior) is not float:\n","        print(\"If using a non-uniform prior:\")\n","        print(\"Please also pass a count vectorizer with the vocabulary parameter set.\")\n","        quit()\n","\n","    l1 = [basic_sanitize(l) for l in l1]\n","    l2 = [basic_sanitize(l) for l in l2]\n","\n","    if cv is None:\n","        cv = CV(\n","            decode_error = 'ignore',\n","            min_df = .1,\n","            max_df = .7,\n","            ngram_range=(1,ngram),\n","            binary = False,\n","            max_features = 1500)\n","    counts_mat = cv.fit_transform(l1+l2).toarray()\n","    # Now sum over languages...\n","    vocab_size = len(cv.vocabulary_)\n","    print(\"Vocab size is {}\".format(vocab_size))\n","    if type(prior) is float:\n","        priors = np.array([prior for i in range(vocab_size)])\n","    else:\n","        priors = prior\n","    z_scores = np.empty(priors.shape[0])\n","    count_matrix = np.empty([2, vocab_size], dtype=np.float32)\n","    count_matrix[0, :] = np.sum(counts_mat[:len(l1), :], axis = 0)\n","    count_matrix[1, :] = np.sum(counts_mat[len(l1):, :], axis = 0)\n","    a0 = np.sum(priors)\n","    n1 = 1.*np.sum(count_matrix[0,:])\n","    n2 = 1.*np.sum(count_matrix[1,:])\n","    print(\"Comparing language...\")\n","    for i in range(vocab_size):\n","        #compute delta\n","        term1 = np.log((count_matrix[0,i] + priors[i])/(n1 + a0 - count_matrix[0,i] - priors[i]))\n","        term2 = np.log((count_matrix[1,i] + priors[i])/(n2 + a0 - count_matrix[1,i] - priors[i]))\n","        delta = term1 - term2\n","        #compute variance on delta\n","        var = 1./(count_matrix[0,i] + priors[i]) + 1./(count_matrix[1,i] + priors[i])\n","        #store final score\n","        z_scores[i] = delta/np.sqrt(var)\n","    index_to_term = {v: k for k, v in cv.vocabulary_.items()}\n","    sorted_indices = np.argsort(z_scores)\n","    return_list = [(index_to_term[i], z_scores[i]) for i in sorted_indices]\n","\n","\n","    # plotting z scores and frequencies\n","    x_vals = count_matrix.sum(axis=0)\n","    y_vals = z_scores\n","    sizes = abs(z_scores) * 2\n","    neg_color, pos_color, insig_color = (colors_d[lang1], colors_d[lang2], '#d8d8d8')\n","    colors = []\n","    annots = []\n","    for i, y in enumerate(y_vals):\n","        if y > sig_val:\n","            colors.append(pos_color)\n","            annots.append(index_to_term[i])\n","        elif y < -sig_val:\n","            colors.append(neg_color)\n","            annots.append(index_to_term[i])\n","        else:\n","            colors.append(insig_color)\n","            annots.append(None)\n","\n","\n","    fig, ax = plt.subplots(figsize=(15,10))\n","    ax.scatter(x_vals, y_vals, c=colors, linewidth=0, alpha = 0.7)\n","\n","    for i, annot in enumerate(annots):\n","        if annot is not None:\n","            if (np.abs(y_vals[i]) > 2.573):\n","                ax.annotate(annot, (x_vals[i], y_vals[i]), color='black', alpha = 1, fontsize=12)\n","\n","    ax.set_xscale('log')\n","    plt.xlabel(\"Word Frequency\")\n","    plt.ylabel(\"z-score (log scale)\")\n","    ax.spines['right'].set_visible(False)\n","    ax.spines['top'].set_visible(False)\n","    ax.title(f'{lang1} vs {lang2}')\n","\n","    legend_elements = [Line2D([0], [0], marker='o', color=neg_color, label=lang1, markersize=8, alpha=0.8, linestyle=\"None\"),\n","                       Line2D([0], [0], marker='o', color=pos_color, label=lang2, markersize=8, alpha=0.8, linestyle=\"None\")]\n","    ax.legend(handles=legend_elements)\n","\n","    plt.savefig(os.path.join(output_path, f'{lang1} vs {lang2}.png'), dpi = 300)\n","\n","    print(f'Most distinctive words for {lang1}')\n","    for i in return_list[:10]:\n","      print(i)\n","    print(f'Most distinctive words for {lang2}')\n","    for i in return_list[-10:]:\n","      print(i)\n","\n","    return return_list, cv.vocabulary_"],"metadata":{"id":"NI-EFts-oZLt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["colors_d = {'Franco': 'tomato',\n","            'unknown': 'plum',\n","            'other': '#053430',\n","            'Franco + unknown': 'palevioletred'}\n","\n","cond = (poems_df['author'] != 'unknown') & (poems_df['author'] != 'Franco')\n","\n","lang_d = {'Franco': poems_df.loc[poems_df['author'] == 'Franco', 'poem'].tolist(),\n","          'unknown': poems_df.loc[poems_df['author'] == 'unknown', 'poem'].tolist(),\n","          'other' : poems_df.loc[cond, 'poem'].tolist()\n","          }"],"metadata":{"id":"4AnFhsZxWjRX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorizer = CV(decode_error = 'ignore', max_df = 0.7, min_df = 0.1, binary = False)\n","\n","output_list, vocabulary = bayes_compare_language('Franco', 'other', figures_dir, cv=vectorizer)\n"],"metadata":{"id":"fH4yYlBUzXo_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorizer = CV(decode_error = 'ignore', max_df = 0.95, min_df = 0.05, binary = False)\n","\n","output_list, vocabulary = bayes_compare_language(unknown, other, figures_dir, cv=vectorizer)"],"metadata":{"id":"KsX_1_2fdbUC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorizer = CV(decode_error = 'ignore', max_df = 0.95, min_df = 0.05, binary = False)\n","\n","output_list, vocabulary = bayes_compare_language(franco, unknown, figures_dir, cv=vectorizer)"],"metadata":{"id":"0rxRzQz19fpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorizer = CV(decode_error = 'ignore', max_df = 0.7, min_df = 0.1, binary = False)\n","\n","output_list, vocabulary = bayes_compare_language(franco+unknown, other, figures_dir, cv=vectorizer)"],"metadata":{"id":"lFDgyBpmqmmq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in output_list[:10] + output_list[-10:]:\n","  print(i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qUGeyLl1_mvM","executionInfo":{"status":"ok","timestamp":1662152645172,"user_tz":240,"elapsed":161,"user":{"displayName":"Federica Bologna","userId":"16530074403271623482"}},"outputId":"fb5fdbf0-38b7-4c68-f7d8-51c4c7e20001"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('bel', -5.234728057618968)\n","('tuo', -5.191988343594773)\n","('occhi', -5.132252565753478)\n","('tua', -4.6647916757456995)\n","('onde', -4.147297522499295)\n","('te', -3.9282323718747176)\n","('sol', -3.8562925507206884)\n","('morte', -3.8459311737603445)\n","('luce', -3.8064597606893398)\n","('volto', -3.7507474879337654)\n","('con', 3.3811552858961984)\n","('senza', 3.637003383629772)\n","('quanto', 3.739699962522934)\n","('ancor', 4.3414427735870005)\n","('vostro', 4.659139367740917)\n","('da', 4.960107606189678)\n","('non', 5.269183151572305)\n","('vi', 5.336727414721941)\n","('mio', 5.4426905357980235)\n","('voi', 5.717610157839146)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NOnW9TWQA4FZ"},"execution_count":null,"outputs":[]}]}