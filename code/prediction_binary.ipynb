{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting predictions for the unknown author from the best binary performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CvDda5pKxeDj"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_predictions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g7KgVPlKttLf"
   },
   "outputs": [],
   "source": [
    "incerto_dir = './'\n",
    "poems_dir = os.path.join(incerto_dir, 'data', 'poems')\n",
    "figures_dir = os.path.join(incerto_dir, 'figures')\n",
    "output_dir = os.path.join(incerto_dir, 'output')\n",
    "classification_path = os.path.join(output_dir, f'binary_classification_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df_split = pd.read_csv(os.path.join(poems_dir, 'poems_split.csv'))\n",
    "p_df_whole = pd.read_csv(os.path.join(poems_dir, 'poems_whole.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "Cn7hRW9BLWBW",
    "outputId": "1ed1e418-f9f1-4432-fcc5-be870594649e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>author</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>ngram type</th>\n",
       "      <th>ngram range</th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>max_f</th>\n",
       "      <th>num_f</th>\n",
       "      <th>scaler</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN</td>\n",
       "      <td>Franco</td>\n",
       "      <td>Count</td>\n",
       "      <td>Char</td>\n",
       "      <td>Bigrams</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>243</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>0.579222</td>\n",
       "      <td>Split</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classifier  author vectorizer ngram type ngram range  max_df  min_df max_f  \\\n",
       "0        kNN  Franco      Count       Char     Bigrams     0.8     0.0  None   \n",
       "\n",
       "   num_f          scaler  f1-score   type  \n",
       "0    243  StandardScaler  0.579222  Split  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.read_csv(classification_path)\n",
    "results_df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models with F1-score (macro avg) > 0.7:\t 769\n",
      "Average f1-score:\t\t\t\t\t 0.77\n",
      "Number of models using split poems:\t\t\t 348\n"
     ]
    }
   ],
   "source": [
    "cond = (results_df['f1-score'] >= 0.7) & (results_df['author'] == 'Franco')\n",
    "franco_top = results_df.loc[cond]\n",
    "print('Number of models with F1-score (macro avg) > 0.7:\\t', len(franco_top))\n",
    "print('Average f1-score:\\t\\t\\t\\t\\t', round(franco_top['f1-score'].mean(), 2))\n",
    "print('Number of models using split poems:\\t\\t\\t', len(franco_top.loc[franco_top['type'] == 'Split']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models with F1-score (macro avg) > 0.7:\t 533\n",
      "Authors the models are trained to classify:\t\t ['Petrarca']\n",
      "Average f1-score:\t\t\t\t\t 0.75\n",
      "Number of models using split poems:\t\t\t 14\n"
     ]
    }
   ],
   "source": [
    "cond = (results_df['f1-score'] >= 0.7) & (results_df['author'] == 'Petrarca')\n",
    "petrarca_top = results_df.loc[cond].copy()\n",
    "print('Number of models with F1-score (macro avg) > 0.7:\\t', len(petrarca_top))\n",
    "print('Authors the models are trained to classify:\\t\\t', petrarca_top['author'].unique())\n",
    "print('Average f1-score:\\t\\t\\t\\t\\t', round(petrarca_top['f1-score'].mean(), 2))\n",
    "print('Number of models using split poems:\\t\\t\\t', len(petrarca_top.loc[petrarca_top['type'] == 'Split']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-XquZeoDJryy",
    "outputId": "e6714f8a-b52f-4cfc-947d-68db572eb47d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models with F1-score (macro avg) > 0.7:\t 8\n",
      "Authors the models are trained to classify:\t\t ['PietroBembo']\n",
      "Average f1-score:\t\t\t\t\t 0.73\n",
      "Number of models using split poems:\t\t\t 8\n"
     ]
    }
   ],
   "source": [
    "cond = (results_df['f1-score'] >= 0.7) & (results_df['author'] != 'Franco') & (results_df['author'] != 'Petrarca')\n",
    "other_top = results_df.loc[cond].copy()\n",
    "print('Number of models with F1-score (macro avg) > 0.7:\\t', len(other_top))\n",
    "print('Authors the models are trained to classify:\\t\\t', other_top['author'].unique())\n",
    "print('Average f1-score:\\t\\t\\t\\t\\t', round(other_top['f1-score'].mean(), 2))\n",
    "print('Number of models using split poems:\\t\\t\\t', len(other_top.loc[other_top['type'] == 'Split']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1310\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>author</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>ngram type</th>\n",
       "      <th>ngram range</th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>max_f</th>\n",
       "      <th>num_f</th>\n",
       "      <th>scaler</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Franco</td>\n",
       "      <td>Count</td>\n",
       "      <td>Char</td>\n",
       "      <td>Bigrams</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>243</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>0.708432</td>\n",
       "      <td>Split</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classifier  author vectorizer ngram type ngram range  max_df  min_df max_f  \\\n",
       "2        SVM  Franco      Count       Char     Bigrams     0.8     0.0  None   \n",
       "\n",
       "   num_f          scaler  f1-score   type  \n",
       "2    243  StandardScaler  0.708432  Split  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_models = results_df.loc[results_df['f1-score'] >= 0.7]\n",
    "print(len(top_models))\n",
    "top_models[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_EByD0V8Mkt",
    "tags": []
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FjkBhHcy2-0l"
   },
   "outputs": [],
   "source": [
    "ngram_range_d = {'Unigrams': (1,1),\n",
    "                 'Bigrams': (2,2),\n",
    "                 'Trigrams': (3,3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WPGytODz8niF"
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "  'RandomForest': RandomForestClassifier(),\n",
    "  'kNN': KNeighborsClassifier(),\n",
    "  'Logit': LogisticRegression(),\n",
    "  'SVM': SVC()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4kOD_dGivQ79"
   },
   "outputs": [],
   "source": [
    "scalers = {'StandardScaler': StandardScaler(),\n",
    "           'L1': Normalizer(norm='l1'),\n",
    "           'L2': Normalizer(norm='l2')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UKhUQ_b9cugY"
   },
   "outputs": [],
   "source": [
    "def build_vectorizer(_typ, _ngram, _max, _min, _max_f):\n",
    "\n",
    "    if _max_f == 'None':\n",
    "        _max_f = None\n",
    "    else:\n",
    "        _max_f = int(_max_f)\n",
    "\n",
    "    if _typ == 'Count':\n",
    "        vec = CountVectorizer(input='content',\n",
    "                    encoding='utf-8',\n",
    "                    lowercase=True,\n",
    "                    analyzer=_ngram[0].lower(),\n",
    "                    ngram_range=_ngram[2],\n",
    "                    max_df=_max,\n",
    "                    min_df=_min,\n",
    "                    max_features=_max_f)\n",
    "\n",
    "    elif _typ == 'TfIdf':\n",
    "        vec = TfidfVectorizer(input='content',\n",
    "                    encoding='utf-8',\n",
    "                    lowercase=True,\n",
    "                    analyzer=_ngram[0].lower(),\n",
    "                    ngram_range=_ngram[2],\n",
    "                    max_df=_max,\n",
    "                    min_df=_min,\n",
    "                    max_features=_max_f,\n",
    "                    norm=None)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "v9Ze2-jyXK7q"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    report_d = classification_report(test_labels, predictions, zero_division=0, output_dict=True)\n",
    "    f1score = report_d['macro avg']['f1-score']\n",
    "    f1score_1 = report_d['1']['f1-score']\n",
    "\n",
    "    return f1score, f1score_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 22:19:59,904\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "if run_predictions == True:\n",
    "    ray.init(num_cpus=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "wSkLZTkJ1vyU",
    "outputId": "70e05a8b-b0ba-4431-f4fd-9df2015cfa6d"
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_predictions(path, row, p_df):\n",
    "\n",
    "  ngram_range = row['ngram range']\n",
    "  author = row['author']\n",
    "  classifier = row['classifier']\n",
    "  scaler = row['scaler']\n",
    "  cv_score = row['f1-score']\n",
    "\n",
    "  if row['ngram type'] == 'Char':\n",
    "      poems = [re.sub(r'\\s+', '', x) for x in p_df['poem'].tolist()]\n",
    "  elif row['ngram type'] == 'Word':\n",
    "      poems = p_df['poem'].tolist()\n",
    "  \n",
    "  # vectorize poems\n",
    "  vectorizer = build_vectorizer(row['vectorizer'], [row['ngram type'], ngram_range, ngram_range_d[ngram_range]], row['max_df'], row['min_df'], row['max_f'])\n",
    "  X = vectorizer.fit_transform(poems)\n",
    "  scaled_X = scalers[scaler].fit_transform(X.toarray())\n",
    "  # final_X = pd.DataFrame(scaled_X, columns=vectorizer.get_feature_names_out()) \n",
    "  # df = p_df[['label', 'author']].merge(final_X, left_index=True, right_index=True)\n",
    "\n",
    "  # select rows for training and testing\n",
    "  known_df = p_df.loc[p_df['author'] != 'Unknown']\n",
    "  known_X = scaled_X[known_df.index, :]\n",
    "  known_y = known_df['author'].map(lambda x: 1 if x==author else 0).tolist()\n",
    "  X_train, X_test, y_train, y_test = train_test_split(known_X, known_y, test_size = 0.25, random_state = 42)\n",
    "  \n",
    "  # training and testing\n",
    "  cl = classifiers[classifier].fit(X_train, y_train)\n",
    "  f1, f1_1 = evaluate(cl, X_test, y_test)\n",
    "\n",
    "  # selecting poems to predict\n",
    "  unknown_df = p_df.loc[p_df['author'] == 'Unknown']\n",
    "  unknown_labels = unknown_df['label'].tolist()\n",
    "  unknown_X = scaled_X[unknown_df.index, :]\n",
    "\n",
    "  predictions = cl.predict(unknown_X)\n",
    "  labeled_predictions = list(zip(unknown_labels, predictions))\n",
    "\n",
    "  for tuple in labeled_predictions:\n",
    "    with open(path, 'a') as csvfile:\n",
    "      csvwriter = csv.writer(csvfile)\n",
    "      csvwriter.writerow((tuple[0], tuple[1],\n",
    "                classifier, author, row['vectorizer'],\n",
    "                row['ngram type'], ngram_range,\n",
    "                row['max_df'], row['min_df'], row['max_f'], row['num_f'], row['scaler'],\n",
    "                row['type'], 'Binary',\n",
    "                cv_score, f1, f1_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dir = os.path.join(incerto_dir, 'output', 'predictions')\n",
    "predictions_path = os.path.join(predictions_dir, 'predictions_binary.csv')\n",
    "if not os.path.exists(predictions_dir):\n",
    "    os.makedirs(predictions_dir)\n",
    "if not os.path.exists(predictions_path):\n",
    "    with open(predictions_path, 'w') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(('label', 'prediction',\n",
    "                            'classifier', 'author', 'vectorizer',\n",
    "                            'ngram_type', 'ngram_range',\n",
    "                            'max_df', 'min_df', 'max_f', 'num_f', 'scaler',\n",
    "                            'poem_type', 'classifier_type',\n",
    "                            'cv_f1-score', 'f1-score', 'f1-score_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = []\n",
    "if run_predictions == True:\n",
    "    for _index, _row in top_models.iterrows():\n",
    "        if _row['type'] == 'Split':\n",
    "            _p_df = p_df_split\n",
    "        elif _row['type'] == 'Whole':\n",
    "            _p_df = p_df_whole\n",
    "        futures.append(do_predictions.remote(predictions_path, _row, _p_df))\n",
    "        # futures.append(do_predictions(predictions_path, _row, _p_df))\n",
    "\n",
    "    results = ray.get(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28780\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>classifier</th>\n",
       "      <th>author</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>ngram_type</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>max_f</th>\n",
       "      <th>num_f</th>\n",
       "      <th>scaler</th>\n",
       "      <th>poem_type</th>\n",
       "      <th>classifier_type</th>\n",
       "      <th>cv_f1-score</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>f1-score_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UA11_1</td>\n",
       "      <td>0</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Franco</td>\n",
       "      <td>Count</td>\n",
       "      <td>Char</td>\n",
       "      <td>Bigrams</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>277</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>Split</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.740272</td>\n",
       "      <td>0.808679</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UA11_2</td>\n",
       "      <td>0</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Franco</td>\n",
       "      <td>Count</td>\n",
       "      <td>Char</td>\n",
       "      <td>Bigrams</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>277</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>Split</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.740272</td>\n",
       "      <td>0.808679</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UA11_3</td>\n",
       "      <td>0</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Franco</td>\n",
       "      <td>Count</td>\n",
       "      <td>Char</td>\n",
       "      <td>Bigrams</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>277</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>Split</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.740272</td>\n",
       "      <td>0.808679</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UA11_4</td>\n",
       "      <td>0</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Franco</td>\n",
       "      <td>Count</td>\n",
       "      <td>Char</td>\n",
       "      <td>Bigrams</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>277</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>Split</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.740272</td>\n",
       "      <td>0.808679</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UA11_5</td>\n",
       "      <td>0</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Franco</td>\n",
       "      <td>Count</td>\n",
       "      <td>Char</td>\n",
       "      <td>Bigrams</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>277</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>Split</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.740272</td>\n",
       "      <td>0.808679</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label  prediction classifier  author vectorizer ngram_type ngram_range  \\\n",
       "0  UA11_1           0        SVM  Franco      Count       Char     Bigrams   \n",
       "1  UA11_2           0        SVM  Franco      Count       Char     Bigrams   \n",
       "2  UA11_3           0        SVM  Franco      Count       Char     Bigrams   \n",
       "3  UA11_4           0        SVM  Franco      Count       Char     Bigrams   \n",
       "4  UA11_5           0        SVM  Franco      Count       Char     Bigrams   \n",
       "\n",
       "   max_df  min_df max_f  num_f          scaler poem_type classifier_type  \\\n",
       "0     0.9     0.0  None    277  StandardScaler     Split          Binary   \n",
       "1     0.9     0.0  None    277  StandardScaler     Split          Binary   \n",
       "2     0.9     0.0  None    277  StandardScaler     Split          Binary   \n",
       "3     0.9     0.0  None    277  StandardScaler     Split          Binary   \n",
       "4     0.9     0.0  None    277  StandardScaler     Split          Binary   \n",
       "\n",
       "   cv_f1-score  f1-score  f1-score_1  \n",
       "0     0.740272  0.808679        0.74  \n",
       "1     0.740272  0.808679        0.74  \n",
       "2     0.740272  0.808679        0.74  \n",
       "3     0.740272  0.808679        0.74  \n",
       "4     0.740272  0.808679        0.74  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.read_csv(predictions_path)\n",
    "print(len(predictions_df))\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jBRl-OO3yLe4",
    "LfE3DPLEyLe4",
    "4acuH_GkyLe4"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "0847cda728ef3e0f335e7e94b5a043d9a0fda1c620343fc6302f7013063303dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
