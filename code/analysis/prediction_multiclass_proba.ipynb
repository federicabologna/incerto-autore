{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting predictions for the unknown author from the best multi-class performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CvDda5pKxeDj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_predictions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g7KgVPlKttLf"
   },
   "outputs": [],
   "source": [
    "incerto_dir = os.path.join(os.getcwd(), '..', '..')\n",
    "poems_dir = os.path.join(incerto_dir, 'data', 'poems')\n",
    "figures_dir = os.path.join(incerto_dir, 'figures')\n",
    "output_dir = os.path.join(incerto_dir, 'output')\n",
    "classification_path = os.path.join(output_dir, 'classification-performance', f'multi_classification_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df_split = pd.read_csv(os.path.join(poems_dir, 'poems_split.csv'))\n",
    "p_df_whole = pd.read_csv(os.path.join(poems_dir, 'poems_whole.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "Cn7hRW9BLWBW",
    "outputId": "1ed1e418-f9f1-4432-fcc5-be870594649e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>ngram type</th>\n",
       "      <th>ngram range</th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>max_f</th>\n",
       "      <th>num_f</th>\n",
       "      <th>scaler</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN</td>\n",
       "      <td>Count</td>\n",
       "      <td>Char</td>\n",
       "      <td>Bigrams</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>259</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>0.113861</td>\n",
       "      <td>Split</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classifier vectorizer ngram type ngram range  max_df  min_df  max_f  num_f  \\\n",
       "0        kNN      Count       Char     Bigrams     0.8     0.0    NaN    259   \n",
       "\n",
       "           scaler  f1-score   type  \n",
       "0  StandardScaler  0.113861  Split  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.read_csv(classification_path)\n",
    "results_df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>ngram type</th>\n",
       "      <th>ngram range</th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>max_f</th>\n",
       "      <th>num_f</th>\n",
       "      <th>scaler</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Logit</td>\n",
       "      <td>Count</td>\n",
       "      <td>Word</td>\n",
       "      <td>Unigrams</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>0.416496</td>\n",
       "      <td>Split</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    classifier vectorizer ngram type ngram range  max_df  min_df   max_f  \\\n",
       "445      Logit      Count       Word    Unigrams     0.8     0.0  1000.0   \n",
       "\n",
       "     num_f          scaler  f1-score   type  \n",
       "445   1000  StandardScaler  0.416496  Split  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_models = results_df.loc[results_df['f1-score'] >= 0.4]\n",
    "print(len(top_models))\n",
    "top_models[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_EByD0V8Mkt",
    "tags": []
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FjkBhHcy2-0l"
   },
   "outputs": [],
   "source": [
    "ngram_range_d = {'Unigrams': (1,1),\n",
    "                 'Bigrams': (2,2),\n",
    "                 'Trigrams': (3,3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WPGytODz8niF"
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "  'RandomForest': RandomForestClassifier(),\n",
    "  'kNN': KNeighborsClassifier(),\n",
    "  'Logit': LogisticRegression(),\n",
    "  'SVM': SVC()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4kOD_dGivQ79"
   },
   "outputs": [],
   "source": [
    "scalers = {'StandardScaler': StandardScaler(),\n",
    "           'L1': Normalizer(norm='l1'),\n",
    "           'L2': Normalizer(norm='l2')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UKhUQ_b9cugY"
   },
   "outputs": [],
   "source": [
    "def build_vectorizer(_typ, _ngram, _max, _min, _max_f):\n",
    "\n",
    "    if _max_f == 'None':\n",
    "        _max_f = None\n",
    "    else:\n",
    "        _max_f = int(_max_f)\n",
    "\n",
    "    if _typ == 'Count':\n",
    "        vec = CountVectorizer(input='content',\n",
    "                    encoding='utf-8',\n",
    "                    lowercase=True,\n",
    "                    analyzer=_ngram[0].lower(),\n",
    "                    ngram_range=_ngram[2],\n",
    "                    max_df=_max,\n",
    "                    min_df=_min,\n",
    "                    max_features=_max_f)\n",
    "\n",
    "    elif _typ == 'TfIdf':\n",
    "        vec = TfidfVectorizer(input='content',\n",
    "                    encoding='utf-8',\n",
    "                    lowercase=True,\n",
    "                    analyzer=_ngram[0].lower(),\n",
    "                    ngram_range=_ngram[2],\n",
    "                    max_df=_max,\n",
    "                    min_df=_min,\n",
    "                    max_features=_max_f,\n",
    "                    norm=None)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "v9Ze2-jyXK7q"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    report_d = classification_report(test_labels, predictions, zero_division=0, output_dict=True)\n",
    "    # print(report_d)\n",
    "    # f1score = report_d['macro avg']['f1-score']\n",
    "    # f1scores = []\n",
    "    # keys = sorted(report_d.keys())\n",
    "    # for key in keys:\n",
    "    #     if type(report_d[key]) == dict and 'avg' not in key:\n",
    "    #         f1scores.append(report_d[key]['f1-score'])\n",
    "\n",
    "    return report_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run_predictions == True:\n",
    "#     ray.init(num_cpus=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "wSkLZTkJ1vyU",
    "outputId": "70e05a8b-b0ba-4431-f4fd-9df2015cfa6d"
   },
   "outputs": [],
   "source": [
    "# @ray.remote\n",
    "def do_predictions(path, row, p_df):\n",
    "  print(row)\n",
    "  ngram_range = row['ngram range']\n",
    "  classifier = row['classifier']\n",
    "  scaler = row['scaler']\n",
    "  cv_score = row['f1-score']\n",
    "\n",
    "  if row['ngram type'] == 'Char':\n",
    "      poems = [re.sub(r'\\s+', '', x) for x in p_df['poem'].tolist()]\n",
    "  elif row['ngram type'] == 'Word':\n",
    "      poems = p_df['poem'].tolist()\n",
    "  \n",
    "  # vectorize poems\n",
    "  vectorizer = build_vectorizer(row['vectorizer'], [row['ngram type'], ngram_range, ngram_range_d[ngram_range]], row['max_df'], row['min_df'], row['max_f'])\n",
    "  X = vectorizer.fit_transform(poems)\n",
    "  scaled_X = scalers[scaler].fit_transform(X.toarray())\n",
    "  # final_X = pd.DataFrame(scaled_X, columns=vectorizer.get_feature_names_out()) \n",
    "  # df = p_df[['label', 'author']].merge(final_X, left_index=True, right_index=True)\n",
    "\n",
    "  # select rows for training and testing\n",
    "  known_df = p_df.loc[p_df['author'] != 'UnknownAuthor']\n",
    "  known_X = scaled_X[known_df.index, :]\n",
    "  known_y = known_df['author'].tolist()\n",
    "  # X_train, X_test, y_train, y_test = train_test_split(known_X, known_y, test_size = 0.25, random_state = 42)\n",
    "  \n",
    "  # training and testing\n",
    "  # cl = classifiers[classifier].fit(X_train, y_train)\n",
    "  # evaluation_report = evaluate(cl, X_test, y_test)\n",
    "  cl = classifiers[classifier].fit(known_X, known_y)\n",
    "\n",
    "  # selecting poems to predict\n",
    "  unknown_df = p_df.loc[p_df['author'] == 'UnknownAuthor']\n",
    "  unknown_labels = unknown_df['label'].tolist()\n",
    "  unknown_X = scaled_X[unknown_df.index, :]\n",
    "\n",
    "  predictions = cl.predict_proba(unknown_X)\n",
    "  authored_predictions = []\n",
    "  for chunk_predictions in predictions:\n",
    "    authored_predictions.append(list(zip(cl.classes_, chunk_predictions)))\n",
    "  # print(authored_predictions)\n",
    "  labeled_predictions = list(zip(unknown_labels, authored_predictions))\n",
    "  # print(labeled_predictions)\n",
    "\n",
    "  for chunk_pred in labeled_predictions:\n",
    "    chunk = chunk_pred[0]\n",
    "    pred = chunk_pred[1]\n",
    "    for au_prob in pred:\n",
    "      au = au_prob[0]\n",
    "      prob = au_prob[1]\n",
    "      csv_row = [chunk, prob,\n",
    "                  classifier, au, row['vectorizer'],\n",
    "                  row['ngram type'], ngram_range,\n",
    "                  row['max_df'], row['min_df'], row['max_f'], row['num_f'], row['scaler'],\n",
    "                  row['type'], 'Multiclass',\n",
    "                  cv_score]#, evaluation_report[au]['f1-score']]\n",
    "      with open(path, 'a') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(csv_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dir = os.path.join(incerto_dir, 'output', 'predictions')\n",
    "predictions_path = os.path.join(predictions_dir, 'predictions_multiclass2.csv')\n",
    "authors = [x+'_f1' for x in sorted(p_df_split[p_df_split['author'] != 'Unknown'].author.unique())]\n",
    "\n",
    "if not os.path.exists(predictions_dir):\n",
    "    os.makedirs(predictions_dir)\n",
    "if not os.path.exists(predictions_path):\n",
    "    row = ['label', 'probability',\n",
    "            'classifier', 'author', 'vectorizer',\n",
    "            'ngram_type', 'ngram_range',\n",
    "            'max_df', 'min_df', 'max_f', 'num_f', 'scaler',\n",
    "            'poem_type', 'classifier_type',\n",
    "            'cv_f1-score']#, 'f1-score'] #+ authors\n",
    "\n",
    "    with open(predictions_path, 'w') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow((row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier              Logit\n",
      "vectorizer              Count\n",
      "ngram type               Word\n",
      "ngram range          Unigrams\n",
      "max_df                    0.8\n",
      "min_df                    0.0\n",
      "max_f                  1000.0\n",
      "num_f                    1000\n",
      "scaler         StandardScaler\n",
      "f1-score             0.416496\n",
      "type                    Split\n",
      "Name: 445, dtype: object\n",
      "classifier              Logit\n",
      "vectorizer              Count\n",
      "ngram type               Word\n",
      "ngram range          Unigrams\n",
      "max_df                    0.9\n",
      "min_df                    0.0\n",
      "max_f                  1000.0\n",
      "num_f                    1000\n",
      "scaler         StandardScaler\n",
      "f1-score              0.41646\n",
      "type                    Split\n",
      "Name: 517, dtype: object\n",
      "classifier              Logit\n",
      "vectorizer              Count\n",
      "ngram type               Word\n",
      "ngram range          Unigrams\n",
      "max_df                    1.0\n",
      "min_df                    0.0\n",
      "max_f                  1000.0\n",
      "num_f                    1000\n",
      "scaler         StandardScaler\n",
      "f1-score              0.41646\n",
      "type                    Split\n",
      "Name: 589, dtype: object\n",
      "classifier              Logit\n",
      "vectorizer              TfIdf\n",
      "ngram type               Word\n",
      "ngram range          Unigrams\n",
      "max_df                    0.8\n",
      "min_df                    0.0\n",
      "max_f                  1000.0\n",
      "num_f                    1000\n",
      "scaler         StandardScaler\n",
      "f1-score             0.416496\n",
      "type                    Split\n",
      "Name: 1093, dtype: object\n",
      "classifier              Logit\n",
      "vectorizer              TfIdf\n",
      "ngram type               Word\n",
      "ngram range          Unigrams\n",
      "max_df                    0.9\n",
      "min_df                    0.0\n",
      "max_f                  1000.0\n",
      "num_f                    1000\n",
      "scaler         StandardScaler\n",
      "f1-score              0.41646\n",
      "type                    Split\n",
      "Name: 1165, dtype: object\n",
      "classifier              Logit\n",
      "vectorizer              TfIdf\n",
      "ngram type               Word\n",
      "ngram range          Unigrams\n",
      "max_df                    1.0\n",
      "min_df                    0.0\n",
      "max_f                  1000.0\n",
      "num_f                    1000\n",
      "scaler         StandardScaler\n",
      "f1-score              0.41646\n",
      "type                    Split\n",
      "Name: 1237, dtype: object\n"
     ]
    }
   ],
   "source": [
    "futures = []\n",
    "if run_predictions == True:\n",
    "    for _index, _row in top_models.iterrows():\n",
    "        if _row['type'] == 'Split':\n",
    "            _p_df = p_df_split\n",
    "        elif _row['type'] == 'Whole':\n",
    "            _p_df = p_df_whole\n",
    "        do_predictions(predictions_path, _row, _p_df)\n",
    "        # futures.append(do_predictions.remote(predictions_path, _row, _p_df))\n",
    "\n",
    "    # results = ray.get(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5040\n",
      "Index(['label', 'probability', 'classifier', 'author', 'vectorizer',\n",
      "       'ngram_type', 'ngram_range', 'max_df', 'min_df', 'max_f', 'num_f',\n",
      "       'scaler', 'poem_type', 'classifier_type', 'cv_f1-score'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>probability</th>\n",
       "      <th>classifier</th>\n",
       "      <th>author</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>ngram_type</th>\n",
       "      <th>ngram_range</th>\n",
       "      <th>max_df</th>\n",
       "      <th>min_df</th>\n",
       "      <th>max_f</th>\n",
       "      <th>num_f</th>\n",
       "      <th>scaler</th>\n",
       "      <th>poem_type</th>\n",
       "      <th>classifier_type</th>\n",
       "      <th>cv_f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UnknownAuthor_1_1</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>Logit</td>\n",
       "      <td>AntonGiacomoCorso</td>\n",
       "      <td>Count</td>\n",
       "      <td>Word</td>\n",
       "      <td>Unigrams</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>Split</td>\n",
       "      <td>Multiclass</td>\n",
       "      <td>0.416496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UnknownAuthor_1_1</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>Logit</td>\n",
       "      <td>BartolomeoZacco</td>\n",
       "      <td>Count</td>\n",
       "      <td>Word</td>\n",
       "      <td>Unigrams</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>Split</td>\n",
       "      <td>Multiclass</td>\n",
       "      <td>0.416496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UnknownAuthor_1_1</td>\n",
       "      <td>0.010046</td>\n",
       "      <td>Logit</td>\n",
       "      <td>CelioMagno</td>\n",
       "      <td>Count</td>\n",
       "      <td>Word</td>\n",
       "      <td>Unigrams</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>Split</td>\n",
       "      <td>Multiclass</td>\n",
       "      <td>0.416496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UnknownAuthor_1_1</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>Logit</td>\n",
       "      <td>DomenicoVenier</td>\n",
       "      <td>Count</td>\n",
       "      <td>Word</td>\n",
       "      <td>Unigrams</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>Split</td>\n",
       "      <td>Multiclass</td>\n",
       "      <td>0.416496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UnknownAuthor_1_1</td>\n",
       "      <td>0.005832</td>\n",
       "      <td>Logit</td>\n",
       "      <td>GiorgioGradenigo</td>\n",
       "      <td>Count</td>\n",
       "      <td>Word</td>\n",
       "      <td>Unigrams</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>Split</td>\n",
       "      <td>Multiclass</td>\n",
       "      <td>0.416496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label  probability classifier             author vectorizer  \\\n",
       "0  UnknownAuthor_1_1     0.030400      Logit  AntonGiacomoCorso      Count   \n",
       "1  UnknownAuthor_1_1     0.000858      Logit    BartolomeoZacco      Count   \n",
       "2  UnknownAuthor_1_1     0.010046      Logit         CelioMagno      Count   \n",
       "3  UnknownAuthor_1_1     0.000414      Logit     DomenicoVenier      Count   \n",
       "4  UnknownAuthor_1_1     0.005832      Logit   GiorgioGradenigo      Count   \n",
       "\n",
       "  ngram_type ngram_range  max_df  min_df   max_f  num_f          scaler  \\\n",
       "0       Word    Unigrams     0.8     0.0  1000.0   1000  StandardScaler   \n",
       "1       Word    Unigrams     0.8     0.0  1000.0   1000  StandardScaler   \n",
       "2       Word    Unigrams     0.8     0.0  1000.0   1000  StandardScaler   \n",
       "3       Word    Unigrams     0.8     0.0  1000.0   1000  StandardScaler   \n",
       "4       Word    Unigrams     0.8     0.0  1000.0   1000  StandardScaler   \n",
       "\n",
       "  poem_type classifier_type  cv_f1-score  \n",
       "0     Split      Multiclass     0.416496  \n",
       "1     Split      Multiclass     0.416496  \n",
       "2     Split      Multiclass     0.416496  \n",
       "3     Split      Multiclass     0.416496  \n",
       "4     Split      Multiclass     0.416496  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.read_csv(predictions_path)\n",
    "print(len(predictions_df))\n",
    "print(predictions_df.columns)\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jBRl-OO3yLe4",
    "LfE3DPLEyLe4",
    "4acuH_GkyLe4"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
